[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/observatorio-de-sueldos-en-Chile/2023-04-21-Observatorio-de-sueldos-en-Chile.html",
    "href": "posts/observatorio-de-sueldos-en-Chile/2023-04-21-Observatorio-de-sueldos-en-Chile.html",
    "title": "Observatorio de sueldos en Chile",
    "section": "",
    "text": "Sueldos en Chile\nLos datos de sueldos en Chile se pueden obtener de la encuesta suplementaria de ingresos (ESI). La última ESI disponible es la del año 2021 y se puede descargar aquí.\nUna vez que hemos descargado los datos podemos acceder a ellos:\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# download the data\ndata_raw = pd.read_csv(\"esi-2021---personas.csv\", encoding=\"ISO-8859-1\", low_memory=False, index_col=0)\ndata_raw.head(3)\n\n\n\n\n\n\n\n\n\nidrph\nnro_linea\nedad\ntramo_edad\nsexo\nparentesco\ncurso\nnivel\ntermino_nivel\nestudia_actual\n...\nmes_central\nano_encuesta\nmes_encuesta\nregion\nr_p_c\nestrato\ntipo\nconglomerado\nid_identificacion\nhogar\n\n\n\n\n1\n14565\n4\n36\n5.0\n1\n4\n2\n4\n2\n2\n...\n11\n2021\n10\n8\n8205\n8200111\n1\n820510025\n102542.0\n1\n\n\n2\n24946\n2\n57\n9.0\n2\n3\n2\n10\n1\n2\n...\n11\n2021\n12\n13\n13123\n13291\n1\n13123000144956\n69274.0\n1\n\n\n3\n41897\n2\n64\n10.0\n2\n2\n3\n8\n1\n2\n...\n11\n2021\n10\n16\n16302\n16300120\n3\n1630220015\n80521.0\n1\n\n\n\n\n3 rows × 292 columns\n\n\n\nPara el análisis, la encuesta considera sólo las personas ocupadas con más de 1 mes en el empleo actual. Podemos acceder al Manual y Guía de Variables ESI y ver que la variable que nos interesa es ocup_ref donde el valor 1 corresponde a los “Ocupados con más de 1 mes en el empleo actual”.\n\n\nCode\ndata = data_raw[data_raw[\"ocup_ref\"] == 1].copy()\n\n\nDel Manual y Guía de Variables ESI, vemos que debemos considerar el factor de expansión que corresponde a la variable fact_cal_esi. De esta forma encontramos que el número de personas ocupadas es 8.243.580.\n\n\nCode\nn_ocupados = data['fact_cal_esi'].sum()\nprint(f\"Número de personas ocupadas: {n_ocupados:,.0f}\".replace(',','.'))\n\n\nNúmero de personas ocupadas: 8.243.580\n\n\nPara calcular el sueldo promedio, según el Manual y Guía de Variables ESI, debemos utilizar la variable ing_t_p que corresponde a “Ingresos del trabajo principal” y debemos utilizar nuevamente el factor de expansión.\n\n\nCode\ndef ingreso_promedio(data):\n    n_ocupados = data['fact_cal_esi'].sum()\n    promedio = (data['ing_t_p']*data['fact_cal_esi']).sum()/n_ocupados\n    return int(np.round(promedio))\nprint(f\"Ingreso promedio mensual: ${ingreso_promedio(data):,.0f}\".replace(',','.'))\n\n\nIngreso promedio mensual: $681.039\n\n\nTanto el número de personas ocupadas como el ingreso promedio coinciden con los valores entregados por el Instituto Nacional de Estadísticas (INE) en la Síntesis de Resultados (ver slide abajo) por lo que vamos por buen camino :-)\n\n\n\nSitio INE ingreso promedio\n\n\nPara calcular el sueldo promedio, diferenciando por hombre/mujer, debemos utilizar la variable sexo.\n\n\nCode\ndata['sexo'] = data['sexo'].map({1: 'hombre', 2: 'mujer'})\nocupados_hombres = data[data['sexo'] == 'hombre']\nocupadas_mujeres = data[data['sexo'] == 'mujer']\nprint(f\"Porcentaje de hombres: {100*ocupados_hombres['fact_cal_esi'].sum()/n_ocupados:.1f}%\")\nprint(f\"Ingreso promedio mensual para hombres: ${ingreso_promedio(ocupados_hombres):,.0f}\".replace(',','.'))\nprint(f\"Porcentaje de mujeres: {100*ocupadas_mujeres['fact_cal_esi'].sum()/n_ocupados:.1f}%\")\nprint(f\"Ingreso promedio mensual para mujeres: ${ingreso_promedio(ocupadas_mujeres):,.0f}\".replace(',','.'))\n\n\nPorcentaje de hombres: 58.2%\nIngreso promedio mensual para hombres: $749.046\nPorcentaje de mujeres: 41.8%\nIngreso promedio mensual para mujeres: $586.178\n\n\nNuevamente los valores coinciden con los valores entregados por el INE (ver slide arriba).\nPara calcular el sueldo promedio, diferenciando por región, debemos utilizar la variable region:\n\n\nCode\nmap_regiones = {\n    1: \"Tarapacá\",\n    2: \"Antofagasta\",\n    3: \"Atacama\",\n    4: \"Coquimbo\",\n    5: \"Valparaíso\",\n    6: \"O'Higgins\",\n    7: \"Maule\",\n    8: \"Biobío\",\n    9: \"La Araucanía\",\n    10: \"Los Lagos\",\n    11: \"Aysén\",\n    12: \"Magallanes\",\n    13: \"Metropolitana\",\n    14: \"Los Ríos\",\n    15: \"Arica y Parinacota\",\n    16: \"Ñuble\",\n    99: \"Región no identificada\"\n}\ndata[\"region\"] = data[\"region\"].map(map_regiones)\nocupados_regiones = data.groupby('region').apply(lambda x: ingreso_promedio(x))\nfor index, value in ocupados_regiones.items():\n    print(f\"Ingreso promedio en la Región de {index}: ${value:,}\".replace(',','.'))\n\n\nIngreso promedio en la Región de Antofagasta: $765.318\nIngreso promedio en la Región de Arica y Parinacota: $582.646\nIngreso promedio en la Región de Atacama: $649.946\nIngreso promedio en la Región de Aysén: $748.998\nIngreso promedio en la Región de Biobío: $574.946\nIngreso promedio en la Región de Coquimbo: $603.089\nIngreso promedio en la Región de La Araucanía: $533.858\nIngreso promedio en la Región de Los Lagos: $552.445\nIngreso promedio en la Región de Los Ríos: $576.430\nIngreso promedio en la Región de Magallanes: $844.329\nIngreso promedio en la Región de Maule: $534.284\nIngreso promedio en la Región de Metropolitana: $780.454\nIngreso promedio en la Región de O'Higgins: $567.721\nIngreso promedio en la Región de Tarapacá: $672.109\nIngreso promedio en la Región de Valparaíso: $601.402\nIngreso promedio en la Región de Ñuble: $543.780\n\n\nLos valores coinciden con los entregados por el INE (ver slide abajo).\n\n\n\nSitio INE ingreso promedio por regiones\n\n\nPara calcular el ingreso mediano (la mitad de las personas ocupadas recibe ingresos menores o iguales al ingreso mediano) para la población total, hombres y mujeres, y por regiones:\n\n\nCode\ndef ingreso_percentil(data, percentil):\n    n_ocupados = data['fact_cal_esi'].sum()\n    data_ordered = data.sort_values(by=\"ing_t_p\", ascending=True)\n    mediano = data_ordered[data_ordered[\"fact_cal_esi\"].cumsum()&gt;n_ocupados*percentil/100.]['ing_t_p'].iloc[0]\n    return int(np.round(mediano))\nprint(f\"Ingreso mediano mensual: ${ingreso_percentil(data,50):,.0f}\".replace(',','.'))\n\n\nIngreso mediano mensual: $457.690\n\n\n\n\nCode\nprint(f\"Ingreso mediano para hombres: ${ingreso_percentil(ocupados_hombres,50):,.0f}\".replace(',','.'))\nprint(f\"Ingreso mediano para mujeres: ${ingreso_percentil(ocupadas_mujeres,50):,.0f}\".replace(',','.'))\n\n\nIngreso mediano para hombres: $500.000\nIngreso mediano para mujeres: $405.348\n\n\n\n\nCode\nocupados_regiones = data.groupby('region').apply(lambda x: ingreso_percentil(x,50))\nfor index, value in ocupados_regiones.items():\n    print(f\"Ingreso mediano en la Región de {index}: ${value:,}\".replace(',','.'))\n\n\nIngreso mediano en la Región de Antofagasta: $570.000\nIngreso mediano en la Región de Arica y Parinacota: $420.000\nIngreso mediano en la Región de Atacama: $506.685\nIngreso mediano en la Región de Aysén: $537.086\nIngreso mediano en la Región de Biobío: $427.307\nIngreso mediano en la Región de Coquimbo: $405.348\nIngreso mediano en la Región de La Araucanía: $397.991\nIngreso mediano en la Región de Los Lagos: $405.348\nIngreso mediano en la Región de Los Ríos: $420.000\nIngreso mediano en la Región de Magallanes: $587.754\nIngreso mediano en la Región de Maule: $400.000\nIngreso mediano en la Región de Metropolitana: $500.000\nIngreso mediano en la Región de O'Higgins: $400.000\nIngreso mediano en la Región de Tarapacá: $469.630\nIngreso mediano en la Región de Valparaíso: $427.642\nIngreso mediano en la Región de Ñuble: $400.000\n\n\nDichos valores coinciden con los valores entregados por el INE (ver slides abajo).\n\n\n\nSitio INE ingreso mediano\n\n\n\n\n\nSitio INE ingreso mediano regiones\n\n\nDe la misma forma como calculamos la mediana podemos calcular los sueldos para todos los percentiles.\n\n\nCode\nimport plotly.graph_objects as go\n\nPERCENTILES = [ingreso_percentil(data,p) for p in range(100)]\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[.01*i for i in range(5,100)], y=PERCENTILES[5:], hovertemplate='Sueldo mensual: %{y:$,.0f}&lt;extra&gt;&lt;/extra&gt;'))\nfig.update_layout(\n    title = f'Distribución de ingresos',\n    yaxis_title = 'Sueldos mensuales',\n    xaxis = dict(\n        tickmode = 'array',\n        tickvals = [.1*i for i in range(11)],\n        ticktext = [f'{10*i}%' for i in range(11)]\n    ),\n    xaxis_tickformat=',.0%',\n    yaxis_tickformat=',.0'.replace(',',','),\n    yaxis = dict(\n        tickmode = 'array',\n        tickvals = [500_000*i for i in range(9)],\n        ticktext = [f'${500_000*i:,}'.replace(',','.') for i in range(9)]\n    ),\n    showlegend=False\n)\nfig.update_layout(\n    hovermode=\"x\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n    )\n)\nfig.show()\n\n\n\n        \n        \n            \n            \n        \n\n\nPuede ver en qué percentil se encuentra su sueldo en esta aplicación.\nDudas o sugerencias pueden hacerlas enviándome un mensaje en Twitter: alonsosilva"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Build a basic LLM chat app with Solara\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nObservatorio de sueldos en Chile\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/observatorio-de-sueldos-en-Chile/2024-04-11-Build_a_basic_LLM_chat_app_with_Solara.html",
    "href": "posts/observatorio-de-sueldos-en-Chile/2024-04-11-Build_a_basic_LLM_chat_app_with_Solara.html",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "In this post, we will build a basic LLM chat app with Solara.\nLet’s start by sending a simple message with the content “Hello!” as a user. To do that we use the ChatBox and ChatMessage components.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=True, name=\"User\"):\n            solara.Markdown(\"Hello!\")\nPage()\n\n\n\n\nYou can modify the user name and/or the message as you please.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=True, name=\"Morpheus\"):\n            solara.Markdown(\"Wake up, Neo...\")\nPage()\n\n\n\n\nYou can also send a message as an assistant.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=False, name=\"Assistant\",):\n            solara.Markdown(\"Hello! How can I assist you today?\")\nPage()\n\n\n\n\nTo have a conversation, we create a reactive variable messages where we will store the messages. To do that we create a list of dictionaries where we will save the roles (for example, user and assistant) and the messages contents.\n\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n\nWe can generate a conversation by adding messages to the reactive variable messages that we previously created and displaying each message one by one.\n\n@solara.component\ndef Page():\n    messages.value = [\n        {\"role\": \"user\", \"content\": \"Hello!\"}, \n        {\"role\": \"assistant\",  \"content\": \"Hello! How can I assist you today?\"},\n    ]\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"Assistant\"\n            ):\n                solara.Markdown(item[\"content\"])\nPage()\n\n\n\n\nLet’s now add the possibility to receive messages from our app user by adding the ChatInput component and a send function that adds the message to the conversation.\n#| source-line-numbers: \"3\"\n#| class-source: \"numberLines\"\n\nimport numpy as np\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"Assistant\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"Assistant\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\n\nWe notice that we are only displaying the message the user sent. Let’s first simulate a conversation by replying exactly the same message we receive from the user. To do that we need to add a response function and a result function that will reply the last message (which will be the one sent by the user) and it will be activated when the user_message_count changes.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\n\n\n\nLet’s now build a Bot that will stream a response message. Let’s first emulate a streamed response with a function that we call response_generator.\n\n# Streamed response emulator\nimport time\nimport random\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hey there! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\nLet’s see that it’s working as expected.\n\nfor chunk in response_generator():\n    print(chunk)\n\nHello! \nHow \ncan \nI \nassist \nyou \ntoday? \n\n\nLet’s create a function that will be adding the chunks to the message\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\n\n\n\n\nimport os\nimport openai\nfrom openai import OpenAI\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\n\nclient = OpenAI()\n\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\n\nfor chunk in response_generator(\"Hello!\"):\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\n\nHello\n!\n How\n can\n I\n assist\n you\n today\n?\n\n\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()"
  },
  {
    "objectID": "posts/observatorio-de-sueldos-en-Chile/2024-04-11-Build_a_basic_LLM_chat_app_with_Solara.html#echobot",
    "href": "posts/observatorio-de-sueldos-en-Chile/2024-04-11-Build_a_basic_LLM_chat_app_with_Solara.html#echobot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "We notice that we are only displaying the message the user sent. Let’s first simulate a conversation by replying exactly the same message we receive from the user. To do that we need to add a response function and a result function that will reply the last message (which will be the one sent by the user) and it will be activated when the user_message_count changes.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()"
  },
  {
    "objectID": "posts/observatorio-de-sueldos-en-Chile/2024-04-11-Build_a_basic_LLM_chat_app_with_Solara.html#streambot",
    "href": "posts/observatorio-de-sueldos-en-Chile/2024-04-11-Build_a_basic_LLM_chat_app_with_Solara.html#streambot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "Let’s now build a Bot that will stream a response message. Let’s first emulate a streamed response with a function that we call response_generator.\n\n# Streamed response emulator\nimport time\nimport random\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hey there! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\nLet’s see that it’s working as expected.\n\nfor chunk in response_generator():\n    print(chunk)\n\nHello! \nHow \ncan \nI \nassist \nyou \ntoday? \n\n\nLet’s create a function that will be adding the chunks to the message\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()"
  },
  {
    "objectID": "posts/observatorio-de-sueldos-en-Chile/2024-04-11-Build_a_basic_LLM_chat_app_with_Solara.html#chatgpt-bot",
    "href": "posts/observatorio-de-sueldos-en-Chile/2024-04-11-Build_a_basic_LLM_chat_app_with_Solara.html#chatgpt-bot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "import os\nimport openai\nfrom openai import OpenAI\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\n\nclient = OpenAI()\n\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\n\nfor chunk in response_generator(\"Hello!\"):\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\n\nHello\n!\n How\n can\n I\n assist\n you\n today\n?\n\n\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()"
  }
]